# Naiad: A timely dataflow system

Naiad (Timely Dataflow), Incremental Dataflow, and Materialize are a family
of streaming database systems that combine low-latency stream processing with
iterative and incremental computation.

Naiad is a distributed analytics database that can best be thought of an
intersection between Kafka and Hadoop. It takes in streams of data and
materializes incremental views on the data. The internal data forms a cyclic,
directed graph.

## Features

It provides the following features:

1. Structured loops (for low latency)
2. Stateful data processors that can process data without global locks (for
   low latency)
3. Notifications when processing is complete (for consistency)

## Pitch

What Naiad does is nothing new: similar systems can be constructed by
stitching features together. The main innovation is that it does this
_efficiently_: it's a single system that covers analytics, replacing
Kafka/Hadoop/batch processing clusters.


## Timely Dataflow

Nodes in timely dataflow make use of a pull-based event model. Kind of like Rust's channels.

### Vertexes (nodes in the graph)

Internally Naiad uses a directed cyclic graph of event emitter nodes
(vertexes) that send messages to each other (edges). There are several kinds
of nodes:

1. input: receives a sequence of messages from an external producer
2. output: emits a sequence of messages back to an external consumer

Input and output vertexes will send a message when no more events for a given
epoch will be sent. Or will send a "close" event when no more messages of
_any_ epoch will be sent. Input vertexes are driven by external sources that
provide the epoch, and "close" event, so it's mostly a way to propagate this
through the system.

### Epochs

Epoch labels are integers and are generated by external producers and sent to
the input nodes. The idea is to create rounds of input data; like
semi-batching, and provide people the ability to quickly react to input, or
wait for batches to terminate before reacting. This enables choosing between
fast incremental compute on each event, or wait for larger cycles to occur.

Note: Would imagine e.g. tweet dashboards to be updated every 1 min or smth.
Feeds could be updated regularly, but not _live_ through this. Something like
that.

### Timestamps and Cycles

Vertexes are organized into _loop contexts_. There are 3 kinds of nodes that
makes up a loop context:

- __ingress node:__ messages going into the context are received here
- __egress node:__ messages leaving the context are sent from here
- __feedback node:__ messages in the context are processed here

Contexts can be nested, but each context must have a __feedback node_ so that
it does work. No empty contexts!

Each message has a timestamp that keeps track of the epoch, and the loop
contexts. That way it can keep track of the current generation, and how it's
progressed throughout the system.

```txt
epoch-[(counter for loop context), ...more counters]
```

Timestamps work like a stack: ingress nodes push a zero to the array.
Feedback nodes increment the counter by 1. Egress nodes pop the latest value
from the array. Put together with the other rules it ensures we always know
how far we are in our graph processing.

- ingress: pushed a `0` to the counter array
- egress: pops the latest value from the counter array
- feedback: increments the latest value of the counter array by 1


### Messaging

Each vertex (node) implements a way to receive messages. Each message has an
event name, data payload, and timestamp. Sending messages is done by calling
a system-wide emitter that sends the message to all listeners registered.

```rust
/// A local node (vertex) that can receive messages.
trait Node: Sized {
    /// Receive an event.
    fn on_recv(&mut self, edge: String, message: String, timestamp: Timestamp);

    /// Receive a notification that no more events from before or at the
    /// timestamp will be emitted.
    fn on_notify(&mut self, timestamp: Timestamp);

    /// Send an event.
    fn send_by(&self, edge: String, message: String, timestamp: Timestamp) {
        // implementation provided
    }

    /// Notify that no more events from the current epoch will be emitted.
    fn notify_at(&self, timestamp: Timestamp) {
        // implementation provided; it also sends the name of the Node.
    }
}
```

The following rules apply:

- Calls to `send_by` map to calls of `on_recv`
- Calls to `notify_at` map to calls of `on_notify`
- Events are queued in the system
- After a timestamp T is sent through `notify_with`, future timestamps for
  `send_by` and `notify_at` must always be later than T. This ensures that we
  can mark the epoch as "done", and work can be processed without needing to
  worry more work may need to be processed again later.


### Point Stamps

Calling `notify_at` by itself is not specific enough to identify an event. We
need to provide more context, so each call includes the name of the Node as
well. For calls to `send_by` we already have an id: the event name + timestamp.

- __send_by:__ the "pointstamp" ID is `(timestamp, edge name)` or as we call
  it `(timestamp, event name)`
- __notify_at:__ the "pointstamp" ID is `(timestamp, vertex name)` -- or
  `(timestamp, node name)`

Or in short:

```txt
type PointStamp = (TimeStamp, Edge | Vertex);
                              |-----------|
                                 Location
```

Pointstamps are considered _could-result-in_ if there exists a path from one
location to another. The timestamp of the pointstamp can be lower or equal to
the target pointstamp.

### Scheduler

The scheduler maintains a set of _active pointstamps_; the edges / vertexes that
correspond to at least one unprocessed event. Per pointstamp it keeps two sets:

- _occurance count_: how many events in the queue relate to the pointstamp?
- _precursor count_: how many edges / vertexes are currently active that might
                     modify this pointstamp in the future?

The counts are updated when the following events occur:

__creating a new pointstamp__
When new pointstamp (edge, vertex) is created: increment the _precursor
  count_ for any pointstamp this may modify in the future. Also set the
  _precursor count_ for the new pointstamp to be however many pointstamps may
  relate to this.

__ lifecycle methods__
- `send_by`: incr the _occurance count_ by 1 for the pointstamp `(TimeStamp, Edge)`
- `on_recv`: decr the _occurance count_ by 1 for the pointstamp `(TimeStamp, Edge)`
- `notify_at`: incr the _occurance count_ by 1 for the pointstamp `(TimeStamp, Vertex)`
- `on_notify`: decr the _occurance count_ by 1 for the pointstamp `(TimeStamp, Vertex)`

__removing pointstamps from the set__

When the _occurance count_ drops to zero, a pointstamp leaves the "active
set". At this point the _precursor count_ for all nodes this could touches on
are also decremented by 1.

__impl nodes__

Each node keeps track of which nodes it relates to. Incrementing /
decrementing is a matter of walking the full tree (incr the relationship you
know, visit that relationship, incr all of its relationships, etc.). This can
be done by storing the full bookkeeping in e.g. a HashMap.

## Resources

- https://blog.acolyer.org/2015/06/12/naiad-a-timely-dataflow-system/
- https://timelydataflow.github.io/differential-dataflow/
